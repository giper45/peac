# 1.0 - 19-01-2026
## Aggiunte
- Informazioni da interviste 
- Aggiunto "Iterative Prompting (clarifying question)

```
“Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions” — Zhang, Knox & Choi (ICLR 2025)
Introduce una metodologia per addestrare LLM a chiedere domande chiarificatrici, usando dati di conversazioni simulate per valutare l’efficacia di tale comportamento.

“Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves” — Deng et al. (2023)
Anche se non è focalizzato unicamente sulle domande chiarificatrici, questa tecnica (Rephrase and Respond) implica che il modello prima riformuli/espanda la domanda umana — un passo utile per ridurre ambiguità e portare il modello ad auto-chiarirsi prima di rispondere.

“Empowering Language Models with Active Inquiry for Deeper Understanding” — Pang et al. (2024)
Introduce LaMAI, un modello che genera attivamente domande informative (“active inquiry”) per ridurre l’ambiguità del contesto prima di rispondere.

“MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning” — Li et al. (2024)
Indirizza direttamente il problema della capacità di porre domande in scenari clinici, sottolineando come modelli che chiedono informazioni aggiuntive possano arrivare a risposte più affidabili.

“AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions for Conversational Search with LLMs” — Siro et al. (2024)
Definisce un framework per far generare e valutare automaticamente delle domande chiarificatrici nell’ambito del conversational search.
```